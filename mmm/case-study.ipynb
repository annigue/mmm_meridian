{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuQtvbG_vILv"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/meridian/blob/main/demo/Meridian_Getting_Started.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/meridian/blob/main/demo/Meridian_Getting_Started.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before: Installation and Loading Packages"
      ],
      "metadata": {
        "id": "vJGOHD8u3RQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install meridian: from PyPI @ latest release\n",
        "#!pip install --upgrade google-meridian[colab,and-cuda]\n",
        "\n",
        "# Install meridian: from PyPI @ specific version\n",
        "#!pip install google-meridian[colab,and-cuda]==1.0.3\n",
        "\n",
        "# Install meridian: from GitHub @HEAD\n",
        "# !pip install --upgrade \"google-meridian[colab,and-cuda] @ git+https://github.com/google/meridian.git\""
      ],
      "metadata": {
        "id": "AxSoD9Cx2dQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import arviz as az\n",
        "\n",
        "import IPython\n",
        "\n",
        "from meridian import constants\n",
        "from meridian.data import load\n",
        "from meridian.data import test_utils\n",
        "from meridian.model import model\n",
        "from meridian.model import spec\n",
        "from meridian.model import prior_distribution\n",
        "from meridian.analysis import optimizer\n",
        "from meridian.analysis import analyzer\n",
        "from meridian.analysis import visualizer\n",
        "from meridian.analysis import summarizer\n",
        "from meridian.analysis import formatter\n",
        "\n",
        "# check if GPU is available\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))"
      ],
      "metadata": {
        "id": "SiSnU-qw2WkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Marketing Mix Model - Case Study with Google's Open Source Meridian**"
      ],
      "metadata": {
        "id": "j8Skgz3a4AAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a marketing mix model case study with Google's open source Meridian. The case study shows an example of a full marketing mix model analysis. All data used in this example is either open source or simulated.\n",
        "\n",
        "The analysis is structured as follow:\n",
        "\n",
        "\n",
        "1.   Project Preparation and Goal Setting\n",
        "2.   Data Collection and Preparation\n",
        "3.   Exploratory Data Analysis (EDA) Data visualization\n",
        "4.   Modeling Model selection\n",
        "5.   Results and Interpretation\n",
        "6.   Recommendations and Decisions Actionable recommendations\n",
        "7.   Presentation and Communication\n",
        "8.   Monitoring and Model Update Monitoring\n",
        "9.   Technical documentation\n",
        "\n",
        "Even though not all steps are essential for the coding part. I would like to add some comments and recommendations. Each step is crucial to implement a successful marketing mix model, since different stakeholders should be involved.\n"
      ],
      "metadata": {
        "id": "uYUWuZ-44jE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Preparation and Goal Setting.\n",
        "\n",
        "\n",
        "*Purpose*\n",
        "\n",
        "The Marketing Mix Model (MMM) analysis historical data of marketing activities and their impact on a KPI, such as sales, orders, web visits, or even brand metrics. In contrast to, for example, attribution, the KPI is not split between marketing channels to calculate the share of impact of each channel. Instead, other relevant factors, such as external factors, are also taken into account. These might include weather, holidays, economic data, or information about competitors' activities.\n",
        "\n",
        "The purpose of MMM is to gain insights into the past performance of marketing channels while considering all these additional factors. The model can then be used for budget optimization or ROI analysis. With knowledge of the \"true\" performance of each marketing channel, shifting the budget from one channel to another might be optimal.\n",
        "\n",
        "*Stakeholder*\n",
        "1. Internal Data: Who are the key contact within the organization to gather all needed data?\n",
        "2. External Data: Who are key contacts for external data? Is it free to use or do you need additional contracts?\n",
        "3. Questions and Results: Who might be interested in results and might they have additional questions to answer within the project scope?\n",
        "\n",
        "*Scope and timeline*\n",
        "\n",
        "Define scope and timeline of the project. Which marketing channel will be considered? Which level of detail (channel, campaign)?\n",
        "What timeline do you expect? Set specific dates each step of the project. Be kind to yourself, everthing will take longer as expected in the first place.\n",
        "\n",
        "*Data Sources*\n",
        "\n",
        "What data is necessaray? Think about all the data you need. Do you have access to databases for marketing data (spend, impressions, grp etc.), sales data. Which external factors might have an impact? Modeling is a iterative process. Even though you collect a lot data, you probably won't need it. Is there anything else that might be relevant for my organization?\n",
        "\n"
      ],
      "metadata": {
        "id": "G54S86J5LuKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection and Preparation\n",
        "\n",
        "a. Data Collection\n",
        "For our case study we consider a organization in FMCG. We have online and offline marketing activies, offline focus is on TV, radio and OOH. There are a lot online activities: SEM, metasearch, display as well as social media.\n",
        "The KPI of interest is sales. All **marketing activities** as well as **sales data** are covered in the Google example data.\n",
        "\n",
        "There are five relevant **competitors**. Their impact will be tested in the model. For the case study with you sample data.\n",
        "\n",
        "External factors are drawn from different sources. For historical weather data, we use an API from \"Deutschen Wetterdienst\". National bank holidays as well as school holidays are prepared with an python package.\n",
        "\n",
        "All data is needed on a weekly level.\n"
      ],
      "metadata": {
        "id": "S5gte3j1pcRr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6GiMzd10EOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Data Collection and Preparation\n",
        "Identify data sources: Gather relevant data (e.g., historical sales data, advertising spend, external factors like weather, seasonal trends, market conditions).\n",
        "Data cleaning: Clean and transform the data (e.g., handling missing values, error correction, normalization).\n",
        "Data integration: Combine the different data sources (e.g., sales, marketing spend, competitor data).\n",
        "Data aggregation: Aggregate the data at an appropriate level (e.g., weekly, monthly) to enable better analysis.\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "Data visualization: Initial visualizations of the data to identify trends, seasonality, and potential relationships.\n",
        "Statistical analysis: Analyze correlations between marketing spend and sales figures.\n",
        "Identify influencing factors: Investigate internal and external factors that affect marketing campaign success (e.g., seasonal effects, holidays).\n",
        "4. Modeling\n",
        "Model selection: Choose the model (e.g., linear regression, logistic regression, time series models, Bayesian models).\n",
        "Model training: Train the model using historical data.\n",
        "Feature engineering: Create additional features that could improve the model (e.g., interactions between marketing channels, lag effects of marketing actions).\n",
        "Hyperparameter tuning: Optimize the model (e.g., selecting model parameters).\n",
        "Validation and testing: Validate the model with a test dataset to assess the model’s quality (e.g., using cross-validation).\n",
        "5. Results and Interpretation\n",
        "Analyze model outputs: Interpret the results of the model (e.g., the impact of each marketing channel on sales, ROI of each campaign).\n",
        "Seasonality and lag effects analysis: Investigate time-related effects and delayed responses to marketing efforts.\n",
        "Benchmarking: Compare the results with other companies or industry benchmarks to validate the findings.\n",
        "Sensitivity analysis: Assess the stability of the model under different assumptions and parameter values.\n",
        "6. Recommendations and Decisions\n",
        "Actionable recommendations: Derive optimization opportunities based on the model results (e.g., reallocating budgets, adjusting marketing strategies).\n",
        "Budget optimization: Provide recommendations for optimal allocation of the marketing budget.\n",
        "Long-term planning: Recommendations for future marketing strategies and actions to improve performance in the long run.\n",
        "7. Presentation and Communication\n",
        "Create report: Summarize results and recommendations in a structured report.\n",
        "Presentation to stakeholders: Present the results to management or other stakeholders (e.g., with visual representations of findings and clear action points).\n",
        "Communicate uncertainties: Highlight the uncertainties and limitations of the model (e.g., assumptions made in the model).\n",
        "8. Monitoring and Model Update\n",
        "Monitoring: Continuously monitor marketing actions and their impact on sales in real-time.\n",
        "Model updates: Regularly update the model to account for new data and changing market conditions.\n",
        "A/B testing: Run experiments to test the effects of specific changes and improve the model.\n",
        "9. Documentation\n",
        "Technical documentation: Provide detailed descriptions of methods, algorithms, and steps used in modeling.\n",
        "Code documentation: Comment the code so that the model can be understood and followed by other team members.\n",
        "Data sources and assumptions: Document the data sources and underlying assumptions used.\n"
      ],
      "metadata": {
        "id": "Ydf1GcLd6DKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weather"
      ],
      "metadata": {
        "id": "xyFngv0A4Ig3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import glob\n",
        "from DWD_hist_weather import tageswerte_land\n",
        "\n",
        "# Pfad für die Pickle-Dateien\n",
        "pickle_directory = '/home/tui/s3synced/csv_data/pickle/'\n",
        "\n",
        "\n",
        "# Lösche alle Pickle-Dateien im Ordner\n",
        "pickle_directory = \"./csv_data/pickle/\"\n",
        "for file in glob.glob(os.path.join(pickle_directory, \"*.pickle\")):\n",
        "    os.remove(file)\n",
        "    print(f\"Gelöscht: {file}\")\n",
        "\n",
        "\n",
        "# Liste der Bundesländer\n",
        "bundeslaender = ['Baden-Württemberg', 'Bayern', 'Berlin', 'Brandenburg', 'Bremen', 'Hamburg',\n",
        "                 'Hessen', 'Mecklenburg-Vorpommern', 'Niedersachsen', 'Nordrhein-Westfalen',\n",
        "                 'Rheinland-Pfalz', 'Saarland', 'Sachsen', 'Sachsen-Anhalt', 'Schleswig-Holstein',\n",
        "                 'Thüringen']\n",
        "\n",
        "# Leeres DataFrame für den Gesamtdatensatz\n",
        "gesamtdatensatz = pd.DataFrame()\n",
        "\n",
        "# Erstelle den Ordner für Pickle-Dateien, falls er nicht existiert\n",
        "os.makedirs(pickle_directory, exist_ok=True)\n",
        "\n",
        "# Iteriere durch alle Bundesländer\n",
        "for bundesland in bundeslaender:\n",
        "    print(f\"\\nVerarbeite Wetterdaten für: {bundesland}...\")\n",
        "    pickle_dateiname = os.path.join(pickle_directory, f\"{bundesland}.pickle\")\n",
        "\n",
        "    try:\n",
        "        # Lade Wetterdaten aus Pickle-Datei\n",
        "        with open(pickle_dateiname, 'rb') as file:\n",
        "            tageswerte = pickle.load(file)\n",
        "        print(f\"Wetterdaten für {bundesland} aus Pickle-Datei geladen.\")\n",
        "    except (FileNotFoundError, EOFError):\n",
        "        # Lade Wetterdaten neu, falls Pickle-Datei nicht existiert\n",
        "        print(f\"Pickle-Datei nicht gefunden. Lade Wetterdaten für {bundesland} neu...\")\n",
        "        try:\n",
        "            tageswerte = tageswerte_land(bundesland, still=True)\n",
        "            # Speichere die Daten als Pickle-Datei\n",
        "            with open(pickle_dateiname, 'wb') as file:\n",
        "                pickle.dump(tageswerte, file)\n",
        "            print(f\"Wetterdaten für {bundesland} gespeichert in {pickle_dateiname}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler beim Abrufen der Wetterdaten für {bundesland}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Füge die Wetterdaten zum Gesamtdatensatz hinzu\n",
        "    tageswerte['Bundesland'] = bundesland\n",
        "    gesamtdatensatz = pd.concat([gesamtdatensatz, tageswerte])\n",
        "\n",
        "# Zeige die ersten Zeilen des kombinierten Datensatzes an\n",
        "print(\"\\nKombinierte Wetterdaten:\")\n",
        "print(gesamtdatensatz.tail())\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "wlK_7A2l4LPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import pandas as pd\n",
        "\n",
        "def aggregiere_woechentlich(gesamtdatensatz):\n",
        "    # Stelle sicher, dass 'Datum' als Spalte vorhanden ist\n",
        "    if gesamtdatensatz.index.name == 'Datum':\n",
        "        gesamtdatensatz = gesamtdatensatz.reset_index()\n",
        "\n",
        "    # Konvertiere Datum zur Wochenspalte (Montag der Woche) und nenne sie 'Datum'\n",
        "    gesamtdatensatz['Datum'] = gesamtdatensatz['Datum'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "\n",
        "    # Gruppierung auf Wochenebene\n",
        "    wochenaggregat = gesamtdatensatz.groupby('Datum').agg({\n",
        "        'TempMean': 'mean',\n",
        "        'TempMax': 'mean',\n",
        "        'TempMin': 'mean',\n",
        "        'HumidityMean': 'mean',\n",
        "        'SunshineDuration': 'mean',\n",
        "        }).reset_index()\n",
        "\n",
        "    # Entferne die Hilfsspalten\n",
        "    df_seas = wochenaggregat[['Datum', 'seas_TempMean', 'seas_TempMax', 'seas_TempMin', 'seas_HumidityMean', 'seas_SunshineDuration']]\n",
        "\n",
        "    return df_seas\n",
        "\n",
        "df_seas = aggregiere_woechentlich(gesamtdatensatz)\n",
        "\n",
        "    # Ausgabe des DataFrames\n",
        "print(\"\\nWöchentlicher Datensatz:\")\n",
        "print(df_seas.head())\n",
        "\n",
        "    # Optional: Speichere den Datensatz\n",
        "#df_seas.to_csv(\"woechentliche_wetterdaten.csv\", index=False)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "gixvY9vQ4kXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import ferien\n",
        "import pandas as pd\n",
        "import time\n",
        "from requests.exceptions import HTTPError\n",
        "\n",
        "def create_pivot_table():\n",
        "    retries = 5\n",
        "    backoff_factor = 2\n",
        "\n",
        "    # Retry mechanism with exponential backoff\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            vacations_list = ferien.all_vacations()\n",
        "            break\n",
        "        except HTTPError as e:\n",
        "            if e.response.status_code == 429:  # Too Many Requests\n",
        "                sleep_time = backoff_factor ** i\n",
        "                print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"An error occurred: \" + str(e))\n",
        "    else:\n",
        "        raise RuntimeError(\"API request failed after several retries.\")\n",
        "\n",
        "    # Tabelle mit Einwohnerzahlen der Bundesländer\n",
        "    population_dict = {\n",
        "        'BW': 11124642, 'BY': 13176989, 'BE': 3677472, 'BB': 2537868,\n",
        "        'HB': 676463, 'HH': 1853935, 'HE': 6295017, 'MV': 1611160,\n",
        "        'NI': 8027031, 'NW': 17924591, 'RP': 4106485, 'SL': 982348,\n",
        "        'SN': 4043002, 'ST': 2169253, 'SH': 2922005, 'TH': 2108863\n",
        "    }\n",
        "\n",
        "    # Liste von Listen für DataFrame erstellen\n",
        "    processed_data = []\n",
        "    for vacation in vacations_list:\n",
        "        start_date = vacation.start\n",
        "        end_date = vacation.end\n",
        "        name = vacation.name\n",
        "        state = vacation.state_code\n",
        "\n",
        "        current_date = start_date\n",
        "        while current_date <= end_date:\n",
        "            week_start_date = current_date - pd.Timedelta(days=current_date.weekday())\n",
        "            week_end_date = week_start_date + pd.Timedelta(days=6)\n",
        "            vacation_days = min((week_end_date - current_date).days + 1, 7)\n",
        "\n",
        "            population = population_dict.get(state, 1)\n",
        "            processed_data.append([week_start_date.date(), name, state, vacation_days, vacation_days * population])\n",
        "            current_date += pd.Timedelta(weeks=1)\n",
        "\n",
        "    df = pd.DataFrame(processed_data, columns=['Datum', 'name', 'state', 'Ferientage', 'hlds_Schoolholidaysgew'])\n",
        "    df['Datum'] = pd.to_datetime(df['Datum'])\n",
        "    df_ferien = df.pivot_table(index='Datum', columns='state', values='Ferientage', aggfunc='sum', fill_value=0)\n",
        "    total_population = sum(population_dict.values())\n",
        "    df_ferien = df_ferien.apply(lambda x: x / population_dict[x.name] * total_population, axis=0)\n",
        "    df_ferien['hlds_Schoolholidaysgew'] = df_ferien.sum(axis=1)\n",
        "\n",
        "    return df_ferien\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    df_ferien = create_pivot_table()\n",
        "    print(df_ferien.tail())\n",
        "\n",
        "df_main = pd.merge(df_main, df_ferien['hlds_Schoolholidaysgew'], on='Datum', how='left')'''\n",
        "\n",
        "'''df_ferien = pd.read_csv('/home/tui/s3synced/csv_data/df_main.csv')\n",
        "\n",
        "df_ferien = df_ferien[['Datum','hlds_Schoolholidaysgew']]\n",
        "df_ferien['Datum'] = pd.to_datetime(df_ferien['Datum'], format='%Y-%m-%d')\n",
        "\n",
        "df_main = pd.merge(df_main, df_ferien, on='Datum', how='left')'''\n"
      ],
      "metadata": {
        "id": "oMK8YN1xKwFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bundesweite Feiertage\n",
        "\n",
        "import holidays\n",
        "import pandas as pd\n",
        "\n",
        "# Erstellen einer Liste von deutschen Feiertagen für die Jahre 2021 bis 2024\n",
        "de_holidays = holidays.Germany(years=[2021, 2022, 2023, 2024])\n",
        "\n",
        "# Erstellen eines Datumsbereichs vom 1. Mai 2021 bis zum 30. April 2024\n",
        "date_range = pd.date_range(start='2021-05-01', end='2024-12-31', freq='D')\n",
        "\n",
        "# Initialisieren eines leeren DataFrame\n",
        "df = pd.DataFrame(index=date_range)\n",
        "df.index = df.index.date\n",
        "\n",
        "# Überprüfen, ob ein Datum ein Feiertag ist, und Zuweisen von 1 für Feiertag und 0 für Nicht-Feiertag\n",
        "df[\"hlds_bankholidays\"] = [1 if date in de_holidays else 0 for date in df.index]\n",
        "\n",
        "# Hinzufügen von Isokalenderwoche und Jahr\n",
        "df[\"iso_year\"] = pd.to_datetime(df.index).isocalendar().year\n",
        "df[\"iso_week\"] = pd.to_datetime(df.index).isocalendar().week\n",
        "\n",
        "# Gruppieren nach Isokalenderwoche und Jahr und Berechnen der Summe der Feiertage pro Woche\n",
        "df_feiertage = df.groupby([\"iso_year\", \"iso_week\"])[\"hlds_bankholidays\"].sum().reset_index()\n",
        "\n",
        "# Berechnen des Montags der jeweiligen Woche\n",
        "df_feiertage[\"Datum\"] = pd.to_datetime(df_feiertage[\"iso_year\"].astype(str) + '-W' + df_feiertage[\"iso_week\"].astype(str) + '-1', format='%G-W%V-%u')\n",
        "\n",
        "# Spalten umbenennen um die Originalnamen beizubehalten\n",
        "df_feiertage = df_feiertage.rename(columns={\"iso_year\": \"Jahr\", \"iso_week\": \"Kalenderwoche\"})\n",
        "\n",
        "# Ergebnis anzeigen\n",
        "df_feiertage\n",
        "\n",
        "\n",
        "'''\n",
        "# Erstellen der Variable 'easter' und 'christmas'\n",
        "df_feiertage['hlds_easter'] = [1 if any(de_holidays.get(date) in ['Good Friday', 'Easter Monday'] for date in pd.date_range(start=week, end=week + pd.DateOffset(days=6))) else 0 for week in df_feiertage['Datum']]\n",
        "# Erstellen der Variable 'easter'\n",
        "# Erstellen der Variable 'christmas'\n",
        "df_feiertage['hlds_christmas'] = [1 if any(\"christmas\" in de_holidays.get(date, \"\") for date in pd.date_range(start=week, end=week + pd.DateOffset(days=6))) else 0 for week in df_feiertage['Datum']]\n",
        "'''\n",
        "df_feiertage.head()\n",
        "# Merge df_main mit df_feiertage anhand der 'Datum'-Spalte\n",
        "df_feiertage['Datum'] = pd.to_datetime(df_feiertage['Datum'], format='%Y-%m-%d')\n",
        "df_main = pd.merge(df_main, df_feiertage, left_on='Datum', right_on='Datum', how='left')\n"
      ],
      "metadata": {
        "id": "O-zZmW-bK0WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqSiFABximWU"
      },
      "source": [
        "# **Introduction to Meridian Demo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckR-pavwis-Q"
      },
      "source": [
        "Welcome to the Meridian end-to-end demo. This simplified demo showcases the fundamental functionalities and basic usage of the library, including working examples of the major modeling steps:\n",
        "\n",
        "\n",
        "<ol start=\"0\">\n",
        "  <li><a href=\"#install\">Install</a></li>\n",
        "  <li><a href=\"#load-data\">Load the data</a></li>\n",
        "  <li><a href=\"#configure-model\">Configure the model</a></li>\n",
        "  <li><a href=\"#model-diagnostics\">Run model diagnostics</a></li>\n",
        "  <li><a href=\"#generate-summary\">Generate model results & two-page output</a></li>\n",
        "  <li><a href=\"#generate-optimize\">Run budget optimization & two-page output</a></li>\n",
        "  <li><a href=\"#save-model\">Save the model object</a></li>\n",
        "</ol>\n",
        "\n",
        "\n",
        "Note that this notebook skips all of the exploratory data analysis and preprocessing steps. It assumes that you have completed these tasks before reaching this point in the demo.\n",
        "\n",
        "This notebook utilizes sample data. As a result, the numbers and results obtained might not accurately reflect what you encounter when working with a real dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_wUxkT7Oh2c0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GicRPam0mUhF"
      },
      "source": [
        "<a name=\"install\"></a>\n",
        "## Step 0: Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDdX9WofM2fx"
      },
      "source": [
        "1\\. Make sure you are using one of the available GPU Colab runtimes which is **required** to run Meridian. You can change your notebook's runtime in `Runtime > Change runtime type` in the menu. All users can use the T4 GPU runtime which is sufficient to run the demo colab, free of charge. Users who have purchased one of Colab's paid plans have access to premium GPUs (such as V100, A100 or L4 Nvidia GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFYRTDuesa1P"
      },
      "source": [
        "2\\. Install the latest version of Meridian, and verify that GPU is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiM0UrN6qbIP"
      },
      "source": [
        "<a name=\"load-data\"></a>\n",
        "## Step 1: Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z18Mo-22x0lY"
      },
      "source": [
        "Load the [simulated dataset in CSV format](https://github.com/google/meridian/blob/main/meridian/data/simulated_data/csv/geo_all_channels.csv) as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZd-ik8NbjK6"
      },
      "source": [
        "1\\. Map the column names to their corresponding variable types. For example, the column names 'GQV' and 'Competitor_Sales' are mapped to `controls`. The required variable types are `time`, `controls`, `population`, `kpi`, `revenue_per_kpi`, `media` and `spend`. If your data includes organic media or non-media treatments, you can add them using `organic_media` and `non_media_treatments` arguments. For the definition of each variable, see\n",
        "[Collect and organize your data](https://developers.google.com/meridian/docs/user-guide/collect-data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sV1ChiEYuyD"
      },
      "outputs": [],
      "source": [
        "coord_to_columns = load.CoordToColumns(\n",
        "    time='time',\n",
        "    geo='geo',\n",
        "    controls=['GQV', 'Competitor_Sales'],\n",
        "    population='population',\n",
        "    kpi='conversions',\n",
        "    revenue_per_kpi='revenue_per_conversion',\n",
        "    media=[\n",
        "        'Channel0_impression',\n",
        "        'Channel1_impression',\n",
        "        'Channel2_impression',\n",
        "        'Channel3_impression',\n",
        "        'Channel4_impression',\n",
        "    ],\n",
        "    media_spend=[\n",
        "        'Channel0_spend',\n",
        "        'Channel1_spend',\n",
        "        'Channel2_spend',\n",
        "        'Channel3_spend',\n",
        "        'Channel4_spend',\n",
        "    ],\n",
        "    organic_media=['Organic_channel0_impression'],\n",
        "    non_media_treatments=['Promo'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JBDZzl80BrY"
      },
      "source": [
        "2\\. Map the media variables and the media spends to the designated channel names intended for display in the two-page HTML output. In the following example,  'Channel0_impression' and 'Channel0_spend' are connected to the same channel, 'Channel0'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qdTSk4a0znn"
      },
      "outputs": [],
      "source": [
        "correct_media_to_channel = {\n",
        "    'Channel0_impression': 'Channel_0',\n",
        "    'Channel1_impression': 'Channel_1',\n",
        "    'Channel2_impression': 'Channel_2',\n",
        "    'Channel3_impression': 'Channel_3',\n",
        "    'Channel4_impression': 'Channel_4',\n",
        "}\n",
        "correct_media_spend_to_channel = {\n",
        "    'Channel0_spend': 'Channel_0',\n",
        "    'Channel1_spend': 'Channel_1',\n",
        "    'Channel2_spend': 'Channel_2',\n",
        "    'Channel3_spend': 'Channel_3',\n",
        "    'Channel4_spend': 'Channel_4',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNr75vQL1Zru"
      },
      "source": [
        "3\\. Load the CSV data using `CsvDataLoader`. Note that `csv_path` is the path to the data file location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udaLGvwl1U8B"
      },
      "outputs": [],
      "source": [
        "loader = load.CsvDataLoader(\n",
        "    csv_path=\"https://raw.githubusercontent.com/google/meridian/refs/heads/main/meridian/data/simulated_data/csv/geo_all_channels.csv\",\n",
        "    kpi_type='non_revenue',\n",
        "    coord_to_columns=coord_to_columns,\n",
        "    media_to_channel=correct_media_to_channel,\n",
        "    media_spend_to_channel=correct_media_spend_to_channel,\n",
        ")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlF5vs8vb8Wn"
      },
      "source": [
        "Note that the simulated data here does not contain reach and frequency. We recommend including reach and frequency data whenever they are available. For information about the advantages of utilizing reach and frequency, see [Bayesian Hierarchical Media Mix Model Incorporating Reach and Frequency Data](https://research.google/pubs/bayesian-hierarchical-media-mix-model-incorporating-reach-and-frequency-data/#:~:text=By%20incorporating%20R%26F%20into%20MMM,based%20on%20optimal%20frequency%20recommendations.). For code snippet for loading reach and frequency data, see [Load geo-level data with reach and frequency](https://developers.google.com/meridian/docs/user-guide/load-geo-data-with-rf)\n",
        "\n",
        "The documentation provides guidance for instances where reach and frequency data is accessible for specific channels. Additionally, for information about how to load other data types and formats, including data with reach and frequency, see [Supported data types and formats](https://developers.google.com/meridian/docs/user-guide/supported-data-types-formats)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO6pDd6f2V1L"
      },
      "source": [
        "<a name=\"configure-model\"></a>\n",
        "## Step 2: Configure the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_mQI7HzxxK4"
      },
      "source": [
        "Meridian uses Bayesian framework and Markov Chain Monte Carlo (MCMC) algorithms to sample from the posterior distribution.\n",
        "\n",
        "1\\. Inititalize the `Meridian` class by passing the loaded data and the customized model specification. One advantage of Meridian lies in its capacity to calibrate the model directly through ROI priors, as described in [Media Mix Model Calibration With Bayesian Priors](https://research.google/pubs/media-mix-model-calibration-with-bayesian-priors/). In this particular example, the ROI priors for all media channels are identical, with each being represented as Lognormal(0.2, 0.9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XNDd7HX1qTn"
      },
      "outputs": [],
      "source": [
        "roi_mu = 0.2     # Mu for ROI prior for each media channel.\n",
        "roi_sigma = 0.9  # Sigma for ROI prior for each media channel.\n",
        "prior = prior_distribution.PriorDistribution(\n",
        "    roi_m=tfp.distributions.LogNormal(roi_mu, roi_sigma, name=constants.ROI_M)\n",
        ")\n",
        "model_spec = spec.ModelSpec(prior=prior)\n",
        "\n",
        "mmm = model.Meridian(input_data=data, model_spec=model_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPQBPlX8cmEv"
      },
      "source": [
        "2\\. Use the `sample_prior()` and `sample_posterior()` methods to obtain samples from the prior and posterior distributions of model parameters. If you are using the T4 GPU runtime this step may take about 10 minutes for the provided data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVB3avRdcRNz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "mmm.sample_prior(500)\n",
        "mmm.sample_posterior(n_chains=7, n_adapt=500, n_burnin=500, n_keep=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUM2V26cspo"
      },
      "source": [
        "For more information about configuring the parameters and using a customized model specification, such as setting different ROI priors for each media channel, see [Configure the model](https://developers.google.com/meridian/docs/user-guide/configure-model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9oECJwUdJTm"
      },
      "source": [
        "<a name=\"model-diagnostics\"></a>\n",
        "## Step 3: Run model diagnostics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSzK6JeMxrV6"
      },
      "source": [
        "After the model is built, you must assess convergence, debug the model if needed, and then assess the model fit.\n",
        "\n",
        "1\\. Assess convergence. Run the following code to generate r-hat statistics. R-hat close to 1.0 indicate convergence. R-hat < 1.2 indicates approximate convergence and is a reasonable threshold for many problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFuc7B86yLvM"
      },
      "outputs": [],
      "source": [
        "model_diagnostics = visualizer.ModelDiagnostics(mmm)\n",
        "model_diagnostics.plot_rhat_boxplot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCwt5SGYxlaE"
      },
      "source": [
        "2\\. Assess the model's fit by comparing the expected sales against the actual sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z4zJtHyyhif"
      },
      "outputs": [],
      "source": [
        "model_fit = visualizer.ModelFit(mmm)\n",
        "model_fit.plot_model_fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76IBQcWLu980"
      },
      "source": [
        "For more information and additional model diagnostics checks, see [Modeling diagnostics](https://developers.google.com/meridian/docs/user-guide/model-diagnostics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGUOFFbCdOtl"
      },
      "source": [
        "<a name=\"generate-summary\"></a>\n",
        "## Step 4: Generate model results & two-page output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puHjkyvZEOEg"
      },
      "source": [
        "To export the two-page HTML summary output, initialize the `Summarizer` class with the model object. Then pass in the filename, filepath, start date, and end date to `output_model_results_summary` to run the summary for that time duration and save it to the specified file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keOpq1qKNbq0"
      },
      "outputs": [],
      "source": [
        "mmm_summarizer = summarizer.Summarizer(mmm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltr4uP80YQe7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbgNaDYpIfQl"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/MyDrive'\n",
        "start_date = '2021-01-25'\n",
        "end_date = '2024-01-15'\n",
        "mmm_summarizer.output_model_results_summary('summary_output.html', filepath, start_date, end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9sBxuvidmr8"
      },
      "source": [
        "Here is a preview of the two-page output based on the simulated data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaUe7uZRfJPm"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(filename='/content/drive/MyDrive/summary_output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PphWMfKdwPIw"
      },
      "source": [
        "For a customized two-page report, model results summary table, and individual visualizations, see [Model results report](https://developers.google.com/meridian/docs/user-guide/generate-model-results-report) and [plot media visualizations](https://developers.google.com/meridian/docs/user-guide/plot-media-visualizations).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqwz2MN5mTq"
      },
      "source": [
        "<a name=\"generate-optimize\"></a>\n",
        "## Step 5: Run budget optimization & generate an optimization report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khCL6Q2sS-iy"
      },
      "source": [
        "You can choose what scenario to run for the budget allocation. In default scenario, you find the optimal allocation across channels for a given budget to maximize the return on investment (ROI).\n",
        "\n",
        "1\\. Instantiate the `BudgetOptimizer` class and run the `optimize()` method without any customization, to run the default library's Fixed Budget Scenario to maximize ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38lhqyLvHf51"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "budget_optimizer = optimizer.BudgetOptimizer(mmm)\n",
        "optimization_results = budget_optimizer.optimize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLOMqDmCRKRO"
      },
      "source": [
        "2\\. Export the 2-page HTML optimization report, which contains optimized spend allocations and ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at7V7YEh_zwZ"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/MyDrive'\n",
        "optimization_results.output_optimization_summary('optimization_output.html', filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq_mcrj1STDU"
      },
      "outputs": [],
      "source": [
        "IPython.display.HTML(filename='/content/drive/MyDrive/optimization_output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIWTubaN0RKC"
      },
      "source": [
        "For information about customized optimization scenarios, such as flexible budget scenarios, see [Budget optimization scenarios](https://developers.google.com/meridian/docs/user-guide/budget-optimization-scenarios). For more information about optimization results summary and individual visualizations, see [optimization results output](https://developers.google.com/meridian/docs/user-guide/generate-optimization-results-output) and [optimization visualizations](https://developers.google.com/meridian/docs/user-guide/plot-optimization-visualizations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m98O3a_TrVg"
      },
      "source": [
        "<a name=\"save-model\"></a>\n",
        "## Step 6: Save the model object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Zjh64YG8Dti"
      },
      "source": [
        "We recommend that you save the model object for future use. This helps you to  avoid repetitive model runs and saves time and computational resources. After the model object is saved, you can load it at a later stage to continue the analysis or visualizations without having to re-run the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kamZpyv8KMh"
      },
      "source": [
        "Run the following codes to save the model object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfaQQ8-fTw0K"
      },
      "outputs": [],
      "source": [
        "file_path='/content/drive/MyDrive/saved_mmm.pkl'\n",
        "model.save_mmm(mmm, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2v_s2uS8PgA"
      },
      "source": [
        "Run the following codes to load the saved model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGUmiYI48epA"
      },
      "outputs": [],
      "source": [
        "mmm = model.load_mmm(file_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}